{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 10:19:34.883066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split , cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                8523 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                8523 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "# sales_predict_df = pd.read_csv(\"./Resources/train_modified.csv\")\n",
    "sales_predict_df = pd.read_csv('../Resources/Train_Output_CSV.csv')\n",
    "sales_predict_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the target column \"Item_Outlet_Sales\" fro the feature DataFrame\n",
    "features_df = sales_predict_df.drop(columns=['Item_Outlet_Sales','Item_Identifier'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Columns: 465 entries, Item_Visibility to Item_Weight_21.35\n",
      "dtypes: float64(2), uint8(463)\n",
      "memory usage: 3.9 MB\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding using pd.get_dummies\n",
    "features_df = pd.get_dummies(features_df, columns=['Outlet_Size' ,'Item_Type' ,'Outlet_Location_Type','Outlet_Establishment_Year','Outlet_Identifier','Outlet_Type', 'Item_Fat_Content','Item_Weight'])\n",
    "\n",
    "# Extract target variables\n",
    "target_df = sales_predict_df['Item_Outlet_Sales']\n",
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = features_df.values\n",
    "y = target_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any missing in X_train_scaled: False\n",
      "Any missing in y_train: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Any missing in X_train_scaled:\", np.isnan(X_train_scaled).any())\n",
    "print(\"Any missing in y_train:\", np.isnan(y_train).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Exhaustive Grid Search\n",
    "\n",
    "In this code snippet, we manually perform hyperparameter tuning for our XGBoost model using exhaustive grid search. The goal is to find the best combination of hyperparameters that yields the highest R-squared value (`R^2`).\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize Variables**: Two variables `best_r2` and `best_params` are initialized to keep track of the best R-squared value and the corresponding parameters.\n",
    "\n",
    "2. **Define Parameters**: Lists of possible values for different hyperparameters like `learning_rate`, `n_estimators`, `max_depth`, `min_child_weight`, and `gamma` are defined.\n",
    "\n",
    "3. **Nested For Loops**: We use nested for loops to iterate through all possible combinations of the defined hyperparameters.\n",
    "\n",
    "4. **Train and Evaluate**: Inside the loops, the XGBoost model is trained with each combination and evaluated using R-squared on the test set.\n",
    "\n",
    "5. **Check and Update**: If the model's R-squared value is better than the previous best, we update `best_r2` and `best_params`.\n",
    "\n",
    "### Code Execution Time:\n",
    "Be cautious, as running this exhaustive search could take a considerable amount of time depending on the number of combinations.\n",
    "\n",
    "### Results:\n",
    "At the end, the code prints the best R-squared value obtained and the hyperparameters that led to it.\n",
    "## Best R^2 score: 0.6253234489918966\n",
    "## Best parameters: {'learning_rate': 0.015, 'n_estimators': 450, 'max_depth': 2, 'min_child_weight': 25, 'gamma': 3}\n",
    "\n",
    "> **Note**: This method is computationally expensive and might not be feasible for very large datasets or a high number of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R^2 score: 0.6253234489918966\n",
      "Best parameters: {'learning_rate': 0.015, 'n_estimators': 450, 'max_depth': 2, 'min_child_weight': 25, 'gamma': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize variables to hold the best parameters and best R^2 score\n",
    "best_r2 = 0\n",
    "best_params = {}\n",
    "\n",
    "# Define possible parameter values (You can extend this list)\n",
    "learning_rates = [0.01, 0.013, 0.015]\n",
    "n_estimators = [400, 430, 450]\n",
    "max_depths = [2, 3, 4]\n",
    "min_child_weights = [20, 23, 25]\n",
    "gammas = [3, 4, 5]\n",
    "\n",
    "# Nested for loops to check various combinations\n",
    "for lr in learning_rates:\n",
    "    for est in n_estimators:\n",
    "        for depth in max_depths:\n",
    "            for weight in min_child_weights:\n",
    "                for gamma in gammas:\n",
    "                    # Create and fit the regressor\n",
    "                    regressor = XGBRegressor(learning_rate=lr, n_estimators=est, max_depth=depth, min_child_weight=weight, gamma=gamma)\n",
    "                    regressor.fit(X_train_scaled, y_train)\n",
    "                    \n",
    "                    # Predict and calculate R^2\n",
    "                    preds = regressor.predict(X_test_scaled)\n",
    "                    current_r2 = r2_score(y_test, preds)\n",
    "                    \n",
    "                    # Check if this R^2 is greater than the previous best\n",
    "                    if current_r2 > best_r2:\n",
    "                        best_r2 = current_r2\n",
    "                        best_params = {'learning_rate': lr, 'n_estimators': est, 'max_depth': depth, 'min_child_weight': weight, 'gamma': gamma}\n",
    "                        \n",
    "print(f\"Best R^2 score: {best_r2}\")\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Nested Loops\n",
    "\n",
    "In this section, we are using nested `for` loops to iterate through various combinations of hyperparameters for our XGBoost model. This manual grid search approach aims to find the combination that yields the highest R-squared value.\n",
    "\n",
    "### Considerations:\n",
    "- **Time Complexity**: Running this code could take a significant amount of time depending on the dataset size and the number of combinations to be tested.\n",
    "- **Optimization Strategy**: The loop iterates through different combinations of `learning_rate`, `n_estimators`, and `max_depth`. After each iteration, the model is trained and its R-squared score is calculated. The hyperparameters that provide the highest R-squared are then selected.\n",
    "\n",
    "### Tips for Speeding Up Execution:\n",
    "1. **Reduce Iterations**: Consider reducing the number of iterations in your loops to make the process faster.\n",
    "2. **Subset of Data**: Use a smaller subset of your training data for quicker execution.\n",
    "3. **Parallel Computing**: Utilize parallel computing capabilities, if available, to speed up the process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imputer = SimpleImputer(strategy=\"mean\")\n",
    "# y_train = imputer.fit_transform(y_train.reshape(-1, 1)).ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refrence \n",
    "# Parameter Tuning: A Complete Guide with Python Codes\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#h-general-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uee XGBoost to define regressor model\n",
    "regressor = XGBRegressor(learning_rate=0.015,\n",
    "    n_estimators=450,\n",
    "    max_depth=2,\n",
    "    min_child_weight=25,\n",
    "    gamma=3,\n",
    "    subsample=.9,\n",
    "    colsample_bytree=.65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Using KFold\n",
    "\n",
    "In this section, we will apply K-Fold Cross-Validation to assess how well the model generalizes to new data.\n",
    "K-Fold Cross-Validation splits the training dataset into 'K' number of subsets, or folds.\n",
    "The model is trained on K-1 of these folds and validated on the remaining one.\n",
    "The process is repeated K times, each time using a different fold as the validation set.\n",
    "The average of all K runs gives us a more robust measure of model performance.\n",
    "\n",
    "We will use `KFold` from scikit-learn to create 5 folds. Our performance metric is \\( R^2 \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.65, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=25, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=450, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.65, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=25, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=450, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.65, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=25, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=450, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a KFold object\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=3)\n",
    "\n",
    "# # Run cross-validation\n",
    "# cv_scores = cross_val_score(regressor, X_train_scaled, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "# # Print the mean R^2 score\n",
    "# print(f'Mean R^2 Score: {np.mean(cv_scores)}')\n",
    "# # Fit the model to the training data\n",
    "regressor.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model on the training data to predict sales\n",
    "sales_data_predictions = regressor.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared value =  0.6238868891857636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Re-run the prediction on modified X_test_scaled\n",
    "sales_data_predictions = regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the R-squared value again\n",
    "r2_sales = metrics.r2_score(y_test, sales_data_predictions)\n",
    "print('R Squared value = ', r2_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Grid Search in Scikit-learn**: A useful guide on how grid search is usually done in scikit-learn.  \n",
    "   - [Scikit-learn GridSearchCV Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "2. **XGBoost Parameters**: Comprehensive guide on XGBoost's parameters, what they mean, and how they affect the model.\n",
    "   - [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "3. **R-squared metric**: Explanation and uses of the R-squared metric in regression problems.\n",
    "   - [Wikipedia: Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "4. **Cross-Validation**: Useful if you'd like to understand why you might opt for K-Fold cross-validation.\n",
    "   - [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "5. **Hyperparameter Optimization**: A broader look at strategies for hyperparameter optimization.\n",
    "   - [Hyperparameter Optimization in Machine Learning Models](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)\n",
    "\n",
    "6. **Computational Cost of Grid Search**: An article discussing the computational expenses and trade-offs of using Grid Search.\n",
    "   - [The Computational Complexity of Grid Search](https://stats.stackexchange.com/questions/29133/the-computational-complexity-of-grid-search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
